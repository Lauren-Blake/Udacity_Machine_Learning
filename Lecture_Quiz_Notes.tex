\documentclass[12pt]{report}
\usepackage{amsmath}
\usepackage{amsfonts}

\begin{document}
\title{Lecture notes on Udacity's ``Introduction to Machine Learning Class''}
\author{Lauren E. Blake}

\maketitle

\begin{flushleft}
\Large{\textit{The class contains 16 lessons as well as a Final Project.}}

\bigskip
\Large{\textit{My code for the exercises and projects can be found on my GitHub Repo: https://github.com/Lauren-Blake/Udacity\_Machine\_Learning.}}
\end{flushleft}

\bigskip 
\section{Lesson 1: Welcome}

\subsection{Introduction I}

\begin{itemize}

\item Lots of applications for machine learning across diverse fields!

\end{itemize}

\subsection{Introduction II}

\begin{itemize}

\item Keep your eyes out for applications of machine learning and data sets that you could use machine learning on. 

\end{itemize}

\subsection{Introduction III}

\begin{itemize}

\item Format: Lectures with quizzes, mini-projects at the end of each lesson.

\item Final project at the end that ties together different aspects of the mini-projects. 

\end{itemize}

\section{Lesson 2: Naive Bayes}

\subsection{ML in the Google Self-Driving Car}

\begin{itemize}

\item Train the self-driving car by showing car how to drive and giving the computer examples of humans driving. This is an example of a supervised classification problem.

\end{itemize}

\subsection{Acerous versus non-Acerous Quiz}

\begin{itemize}

\item Gave example of acerous animals and non-acerous examples. Then asked whether we thought a horse was acerous or not. 

\item Answer: A horse is acerous (lacking horns or antlers).  

\end{itemize}

\subsection{Supervised Classification Examples Quiz}

\begin{itemize}

\item Need to have training data and then making predictions, recommendations, etc. based on the training set. 

\end{itemize}

\subsection{Features and Labels Musical Example Quiz}

\begin{itemize}

\item With a song, features could be tempo, intensity, gender of person singing it, etc. Labels are whether a person likes the song or not. 

\end{itemize}

\subsection{Features Visualization Quiz}

\begin{itemize}

\item Answer: She likes those because they are close to the other data points representing songs that the person also likes.

\end{itemize}

\subsection{Classification by Eye Quiz}

\begin{itemize}

\item Answer: Unclear because the new data point is close to two different labels. 

\end{itemize}

\subsection{Intro To Stanley Terrain Classification}

\begin{itemize}

\item Features: speed and ruggedness

\end{itemize}

\subsection{Speed Scatterplot: Grade and Bumpiness Quiz}

\begin{itemize}

\item Answer: From the picture, we can see that the terrain looks flat and smooth, particularly relative to the other pictures. 

\end{itemize}

\subsection{Speed Scatterplot 2}

\begin{itemize}

\item Answer: From the picture, we can see that the terrain looks very steep and medium bumpy. 

\end{itemize}

\subsection{Speed Scatterplot 3}

\begin{itemize}

\item Answer: From the picture, we can see that the terrain looks flat and very. 

\end{itemize}

\subsection{From Scatterplots to Predictions}

\begin{itemize}

\item Answer: The points look closer to the blue circles than the red X's. 

\end{itemize}

\subsection{From Scatterplots to Predictions 2}

\begin{itemize}

\item Answer: Unclear because the points look equally close to the blue circles than the red X's. 

\end{itemize}

\subsection{Scatterplots to Decision Surface Quiz}

\begin{itemize}

\item A decision surface parses out the training data into different features so that a data point falling on one side of the decision surface has one label and on the other side, a different label. 

\item Answer: Red cross because it is on the same side as the training data with red crosses. 

\end{itemize}

\subsection{A Good Linear Decision Surface}

\begin{itemize}

\item When the decision surface is a straight line, it is a ``linear'' decision surface.

\item Answer: Select the line that clearly and consistently separates the red crosses from the blue circles.  

\end{itemize}

\subsection{Transition into Using Naive Bayes}

\begin{itemize}

\item Naive Bayes is a common algorithm to find the decision surface. 

\end{itemize}

\subsection{NB Decision Boundary in Python}

\begin{itemize}

\item Have 750 training data points and make a decision boundary. 

\end{itemize}

\subsection{Getting Started with sklearn}

\begin{itemize}

\item Documentation on Naive Bayes with derivation and code 

\item clf = GaussianNB() \# Create Gaussian classifier

\item clf.fit(features, labels) \# Fit Gaussian classifier

\item clf.predict \#Give it a point and get out a label

\end{itemize}

\subsection{Gaussian NB Example}

\begin{itemize}

\item Goes through each line of code in the example section of http://scikit-learn.org/stable/modules/generated/sklearn.naive\_bayes.GaussianNB.html. 

\end{itemize}

\subsection{GaussianNB Deployment on Terrain Data}

\begin{itemize}

\item Answer: Add the following code: \\

\#Specify classifier type \\
clf = GaussianNB() \\

\bigskip

\# Fit the decision boundary \\
clf.fit(features\_train, labels\_train) \\

\end{itemize}

\subsection{Finding Naive Bayes Accuracy}

\begin{itemize}

\item accuracy = number of points classified correctly / all points in the test set

\item Answer: See Lesson\_2\_Section\_20\_Quiz:Calculating\_NB\_Accuracy on my Udacity Machine Learning GitHub repo. 

\end{itemize}

\subsection{Training and Testing Data}

\begin{itemize}

\item Important to train and test on different data sets (need to generalize to new data sets)

\end{itemize}

\subsection{Unpacking NB Using Bayes Rule}

\begin{itemize}

\item What is Naive Bayes?

\end{itemize}

\subsection{Bayes Rule}

\subsection{Cancer Test}

\begin{itemize}

\item Answer: We can see in from the diagram that the probability that someone with a positive cancer test actually has the disease is approximately 8\%. 

\end{itemize}

\subsection{Prior and Posterior}

\begin{itemize}

\item Answers: 0.009 and  0.099

\end{itemize}

\subsection{Normalizing 1}

\begin{itemize}

\item Answer: The normalizing constant is 0.009+0.099 = 0.108. 

\end{itemize}

\subsection{Normalizing 2}

\begin{itemize}

\item Answer: 0.08333. Divide the cancer joint by the normalizing constant to get the posterior. 

\end{itemize}

\subsection{Normalizing 3}

\begin{itemize}

\item Answer: 0.916666. Divide the non-cancer joint by the normalizing constant to get the posterior. 

\end{itemize}

\subsection{Total Probability Quiz}

\begin{itemize}

\item Answer: 1. Add the answers from Normalizing 2 and Normalizing 3 together.

\end{itemize}

\subsection{Bayes Rule Diagram}

\begin{equation}
Total probability = 1 = P \big( \frac{C \mid Pos}{Normalizing constant} \big) + P\big( \frac{non-C \mid Pos}{Normalizing constant} \big)
\end{equation}


\subsection{Bayes Rule for Classification}

\begin{itemize}

\item Who is the person that is sending the email based on the words that they used? 

\end{itemize}

\subsection{Chris or Sara Quiz}

\begin{itemize}

\item Answer 1: Sara because the email contains words that she uses with higher probability than Chris. 

\item Answer 2: Chris because the email contains words that she uses with higher probability than Sara. 

\end{itemize}

\subsection{Posterior probabilities}

\begin{itemize}

\item Answers: 0.5714; 1-0.5714; First find 

\begin{equation}
Prob \frac{\big(Chris \mid Email contains the words ``life deal'' \big)}{Constant}
\end{equation}
Then, find its complement. 

\end{itemize}

\subsection{Bayesian probabilities on your own}

\begin{itemize}
\item Prob \big(Chris $\mid$ Email contains the words ``love deal'' \big) = \\

Prob \big(``love deal'' $\mid$ C \big)*P\big(C \big) divided by a normalizing constant \\ 

where the constant is Prob \big(``love deal'' $\mid$ C \big) P\big(C) + Prob \big(``love deal'' $\mid$ S \big) P\big(S) \\

Then, take the complement.

\item Answers: 0.5555; 0.4444. 

\end{itemize}

\subsection{Why Is Naive Bayes Naive}

\begin{itemize}

\item Don't see underlying process (e.g. who is using the words) but get to see the outcome (e.g. the words that the person used). 

\item Answer: Word order is being ignored in Bayes Theorem whereas the words used and the length of the message (in terms of which words are used) are used. 

\end{itemize} 

\subsection{Naive Bayes Strengths and Weaknesses}

\begin{itemize}

\item Pros: Easy to implement, efficient

\item Cons: Can break (e.g. phrases with distinct meanings)

\item Good for text classification because can treat each word as a feature. 

\end{itemize}

\subsection{Congrats on Learning Naive Bayes}

\subsection{Lesson 2 Naive Bayes Mini-Project}

\begin{itemize}

\item See code in ''/Udacity\_Machine\_Learning/Lesson\_2\_Naive\_Bayes\_Mini\_Project\_Code.py''

\item: Answer to Quiz on Author ID Accuracy: no. of Chris training emails: 7936, no. of Sara training emails: 7884. Accuracy: 0.973833902162

\item Answer to Quiz on Timing Your NB Classifier: training time: 1.417 s, predicting time: 0.162 s. Training time is greater than the prediction time. 

\end{itemize}


\section{SVM, Support Vector Machines}

\subsection{Welcome to SVM}

\begin{itemize}

\item SVM is a very popular algorithm

\end{itemize}

\subsection{Quiz: Separating a Line}

\begin{itemize} 

\item SVM takes in data from 2+ classes as input and draws a line to separate the classes

\item Answer: The diagonal line that separates the Xs and the Os.

\end{itemize}

\subsection{Quiz: Choosing Between Separating Lines}

\begin{itemize} 

\item Answer: The vertical line is the best separator.

\end{itemize}


\subsection{Quiz: Choosing Between Separating Lines}

\begin{itemize} 

\item Want to choose a line that maximizes the distances to the nearest points in either class. Margin- maximizes distance to the nearest point.  

\item Answer: Something else because it maximizes the distances to the nearest points in either class (to be the most robust to classification errors). 

\end{itemize}

\subsection{Quiz: Practice with Margins}

\begin{itemize} 

\item Answer: The middle line maximizes the margin. It is the most robust to classification errors. 

\end{itemize}

\subsection{Quiz: SVMs and Tricky Data Distributions}

\begin{itemize} 

\item Answer: The line that fully separates the red from the blue points (diagonally downward). 

\item SVM prioritizes correct classification over maximizing the margin. 

\end{itemize}

\subsection{Quiz: SVM Response to Outliers}

\begin{itemize} 

\item What happens when no decision surface exists that completely separates the classes of data?

\item Answer: Do the best it can

\end{itemize}

\subsection{Quiz: SVM Outlier Practice}

\begin{itemize} 

\item SVM ignores extreme outliers. 

\item Answer: The line on the right is the best separator. 

\end{itemize}

\subsection{Handoff to Katie}

\begin{itemize} 

\item Making your own SVM

\end{itemize}

\subsection{SVM in SKlearn}

\begin{itemize}

\item Code: Import statement, training data, training features, create classifier, fit classifier, do a prediction.

\item Important: import svm, classifier is svm.SVC()

\end{itemize}

\subsection{SVM Decision Boundary}

\begin{itemize}

\item Unlike with our Naive Bayes classifier, the SVM decision boundary will be a straight line. 

\end{itemize}

\subsection{SVM Coding Up the SVM}

\begin{itemize}

\item Answer: See "Lesson\_3\_Section\_12\_Coding\_Up\_the\_SVM for code. Also, the accuracy is 0.92, so it does better than the Naive Bayes classifier. 

\end{itemize}


\subsection{Nonlinear SVMs}

\begin{itemize}

\item SVM can do complicated shapes in the decision boundary. 

\end{itemize}


\subsection{Quiz: Nonlinear Data}

\begin{itemize}

\item Answer: No. Given our definition so far, SVMs will not separate this dataset.

\end{itemize}

\subsection{Quiz: A New Feature}

\begin{itemize}

\item Can put x squared and y squared into SVM in addition to x and y inputs. 

\item Answer: Yes, this is now separable. 

\end{itemize}


\subsection{Visualizing A New Feature}

\begin{itemize}

\item Can do a linear transformation, which means we can look at it in a new coordinate system. 

\end{itemize}

\subsection{Quiz: Separating with the New Feature}

\begin{itemize}

\item Answer: Yes, the data classes are now separable. 

\end{itemize}

\subsection{Quiz: Practice Making a New Feature}

\begin{itemize}

\item Answer: Using the absolute value of X makes the data classes separable. 

\end{itemize}


\subsection{Kernel Trick}

\begin{itemize}

\item Use kernels to change x,y input space to a much larger input (high dimensional) space. And this can lead to a non-linear separation for x, y

\end{itemize}


\subsection{Quiz: Playing Around with Kernel Trick}

\begin{itemize}

\item How to create an SVC and specify a kernel type

\item Answer: All + more. There are many kernel types. 

\end{itemize}


\subsection{Quiz: Kernel and Gamma}

\begin{itemize}

\item Parameters in machine learning- arguments passed when you create your classifier (prior to fitting)

\item Parameters for an SVM- kernel (e.g. linear, rbf); C; gamma

\item Answer: The plot on the far right best represents an SVM with a linear kernel and a gamma = 1.0. 

\end{itemize}

\subsection{Quiz: SVM C Parameter}

\begin{itemize} 

\item C- controls the tradeoff between a smooth decision boundary and classifying training points correctly. 

\item Answer: A smaller value of C will cause the optimizer to look for a larger margin (even if this means that the decision boundary will misclassify more points. Therefore, a large C means that you expect that you will get more training points correct. 

\end{itemize}

\subsection{Quiz: Gamma Parameter}

\begin{itemize}

\item Gamma- defines how far the influence of a single training example reaches. Low values = far reach (even the points far from the potential decision boundary get taken into account when evaluating where to put the decision boundary) and high values = close. 

\end{itemize}

\subsection{Quiz: Overfitting}

\begin{itemize}

\item Overfitting is a common problem in machine learning

\item Answer: C, Gamma, and the Kernel type are all parameters that can impact fit/overfitting.

\end{itemize}

\subsection{SVM Strengths and Weaknesses}

\begin{itemize}

\item SVM works well in complicated domains where there is a clear margin of separation.

\item SVM doesn't work well in large data sets or data sets with lots of noise (e.g. overlapping classes)

\end{itemize}

\subsection{Lesson 2 Naive Bayes Mini-Project}

\begin{itemize}

\item See code in ''/Udacity\_Machine\_Learning/Lesson\_3\_SVM\_Mini\_Project\_Code.py''

\item Answer to Quiz on SVM Author ID Accuracy: The accuracy is 0.984072810011. 

\item Answer to Quiz on SVM Author ID Timing: The training time is 144.617 s and the predicting time: 14.203 s. Therefore, SVM is slower than Naive Bayes in this case. 

\item Answer to Quiz on A Smaller Training Set: The accuracy is 0.884527872582. 

\item Answer to Quiz on Speed-Accuracy Tradeoff: The two cases that happen in real time, flagging credit card fraud and voice recognition. 

\item Answer to Quiz on Deploy an RBF Kernel: Interestingly, with this more complex kernel, our accuracy is lower, 0.616040955631. 

\item Answer to Quiz on Optimize C Parameter: The accuracy when C = 10 is 0.616040955631, C = 100 is 0.616040955631, when C = 1000 is 0.821387940842, when C = 10000 is 0.892491467577. This means that the accuracy is greatest when C is highest (10000). 

\item Answer to Quiz on Accuracy after Optimizing C: The accuracy when C = 10000 is 0.892491467577. Based on the definition of C given earlier in the lesson, greater C equals more complex decision boundaries. 

\item Answer to Quiz on Optimized RBF vs. Linear: The accuracy of the optimized RBF is 0.990898748578. 

\item Answer to Quiz on Extracting Predictions from An SVM: The SVM predicts 1 for element 10, 0 for element 26, and 1 for element 50.

\item Answer to Quiz on How Many Chris Emails Predicted: He is expected to have authored X emails. Note: when the partial training set is used, this answer is 1018 emails. 

\end{itemize}

\subsection{Final Thoughts on Deploying SVM}

\begin{itemize}

\item Generally, Naive Bayes classifiers are better for text than SVM.

\end{itemize}

\section{Lesson 4}
\subsection{Lesson 4 Unit 1}

\begin{itemize}

\item Check

\end{itemize}



\end{document}
