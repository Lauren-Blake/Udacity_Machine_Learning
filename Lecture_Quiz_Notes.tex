\documentclass[12pt]{report}
\usepackage{amsmath}
\usepackage{amsfonts}

\begin{document}

\title{Lecture notes on Udacity's ``Introduction to Machine Learning Class''}
\author{Lauren E. Blake}

\maketitle

\begin{flushleft}
\Large{\textit{The class contains 16 lessons as well as a Final Project.}}

\bigskip
\Large{\textit{My code for the exercises and projects can be found on my GitHub Repo: https://github.com/Lauren-Blake/Udacity\_Machine\_Learning.}}
\end{flushleft}

\bigskip 
\section{Lesson 1: Welcome}

\subsection{Introduction I}

\begin{itemize}

\item Lots of applications for machine learning across diverse fields!

\end{itemize}

\subsection{Introduction II}

\begin{itemize}

\item Keep your eyes out for applications of machine learning and data sets that you could use machine learning on. 

\end{itemize}

\subsection{Introduction III}

\begin{itemize}

\item Format: Lectures with quizzes, mini-projects at the end of each lesson.

\item Final project at the end that ties together different aspects of the mini-projects. 

\end{itemize}

\section{Lesson 2: Naive Bayes}

\subsection{ML in the Google Self-Driving Car}

\begin{itemize}

\item Train the self-driving car by showing car how to drive and giving the computer examples of humans driving. This is an example of a supervised classification problem.

\end{itemize}

\subsection{Acerous versus non-Acerous Quiz}

\begin{itemize}

\item Gave example of acerous animals and non-acerous examples. Then asked whether we thought a horse was acerous or not. 

\item Answer: A horse is acerous (lacking horns or antlers).  

\end{itemize}

\subsection{Supervised Classification Examples Quiz}

\begin{itemize}

\item Need to have training data and then making predictions, recommendations, etc. based on the training set. 

\end{itemize}

\subsection{Features and Labels Musical Example Quiz}

\begin{itemize}

\item With a song, features could be tempo, intensity, gender of person singing it, etc. Labels are whether a person likes the song or not. 

\end{itemize}

\subsection{Features Visualization Quiz}

\begin{itemize}

\item Answer: She likes those because they are close to the other data points representing songs that the person also likes.

\end{itemize}

\subsection{Classification by Eye Quiz}

\begin{itemize}

\item Answer: Unclear because the new data point is close to two different labels. 

\end{itemize}

\subsection{Intro To Stanley Terrain Classification}

\begin{itemize}

\item Features: speed and ruggedness

\end{itemize}

\subsection{Speed Scatterplot: Grade and Bumpiness Quiz}

\begin{itemize}

\item Answer: From the picture, we can see that the terrain looks flat and smooth, particularly relative to the other pictures. 

\end{itemize}

\subsection{Speed Scatterplot 2}

\begin{itemize}

\item Answer: From the picture, we can see that the terrain looks very steep and medium bumpy. 

\end{itemize}

\subsection{Speed Scatterplot 3}

\begin{itemize}

\item Answer: From the picture, we can see that the terrain looks flat and very. 

\end{itemize}

\subsection{From Scatterplots to Predictions}

\begin{itemize}

\item Answer: The points look closer to the blue circles than the red X's. 

\end{itemize}

\subsection{From Scatterplots to Predictions 2}

\begin{itemize}

\item Answer: Unclear because the points look equally close to the blue circles than the red X's. 

\end{itemize}

\subsection{Scatterplots to Decision Surface Quiz}

\begin{itemize}

\item A decision surface parses out the training data into different features so that a data point falling on one side of the decision surface has one label and on the other side, a different label. 

\item Answer: Red cross because it is on the same side as the training data with red crosses. 

\end{itemize}

\subsection{A Good Linear Decision Surface}

\begin{itemize}

\item When the decision surface is a straight line, it is a ``linear'' decision surface.

\item Answer: Select the line that clearly and consistently separates the red crosses from the blue circles.  

\end{itemize}

\subsection{Transition into Using Naive Bayes}

\begin{itemize}

\item Naive Bayes is a common algorithm to find the decision surface. 

\end{itemize}

\subsection{NB Decision Boundary in Python}

\begin{itemize}

\item Have 750 training data points and make a decision boundary. 

\end{itemize}

\subsection{Getting Started with sklearn}

\begin{itemize}

\item Documentation on Naive Bayes with derivation and code 

\item clf = GaussianNB() \# Create Gaussian classifier

\item clf.fit(features, labels) \# Fit Gaussian classifier

\item clf.predict \#Give it a point and get out a label

\end{itemize}

\subsection{Gaussian NB Example}

\begin{itemize}

\item Goes through each line of code in the example section of http://scikit-learn.org/stable/modules/generated/sklearn.naive\_bayes.GaussianNB.html. 

\end{itemize}

\subsection{GaussianNB Deployment on Terrain Data}

\begin{itemize}

\item Answer: Add the following code: \\

\#Specify classifier type \\
clf = GaussianNB() \\

\bigskip

\# Fit the decision boundary \\
clf.fit(features\_train, labels\_train) \\

\end{itemize}

\subsection{Finding Naive Bayes Accuracy}

\begin{itemize}

\item accuracy = number of points classified correctly / all points in the test set

\item Answer: See Lesson\_2\_Section\_20\_Quiz:Calculating\_NB\_Accuracy on my Udacity Machine Learning GitHub repo. 

\end{itemize}

\subsection{Training and Testing Data}

\begin{itemize}

\item Important to train and test on different data sets (need to generalize to new data sets)

\end{itemize}

\subsection{Unpacking NB Using Bayes Rule}

\begin{itemize}

\item What is Naive Bayes?

\end{itemize}

\subsection{Bayes Rule}

\subsection{Cancer Test}

\begin{itemize}

\item Answer: We can see in from the diagram that the probability that someone with a positive cancer test actually has the disease is approximately 8\%. 

\end{itemize}

\subsection{Prior and Posterior}

\begin{itemize}

\item Answers: 0.009 and  0.099

\end{itemize}

\subsection{Normalizing 1}

\begin{itemize}

\item Answer: The normalizing constant is 0.009+0.099 = 0.108. 

\end{itemize}

\subsection{Normalizing 2}

\begin{itemize}

\item Answer: 0.08333. Divide the cancer joint by the normalizing constant to get the posterior. 

\end{itemize}

\subsection{Normalizing 3}

\begin{itemize}

\item Answer: 0.916666. Divide the non-cancer joint by the normalizing constant to get the posterior. 

\end{itemize}

\subsection{Total Probability Quiz}

\begin{itemize}

\item Answer: 1. Add the answers from Normalizing 2 and Normalizing 3 together.

\end{itemize}

\subsection{Bayes Rule Diagram}

\begin{equation}
Total probability = 1 = P \big( \frac{C \mid Pos}{Normalizing constant} \big) + P\big( \frac{non-C \mid Pos}{Normalizing constant} \big)
\end{equation}


\subsection{Bayes Rule for Classification}

\begin{itemize}

\item Who is the person that is sending the email based on the words that they used? 

\end{itemize}

\subsection{Chris or Sara Quiz}

\begin{itemize}

\item Answer 1: Sara because the email contains words that she uses with higher probability than Chris. 

\item Answer 2: Chris because the email contains words that she uses with higher probability than Sara. 

\end{itemize}

\subsection{Posterior probabilities}

\begin{itemize}

\item Answers: 0.5714; 1-0.5714; First find 

\begin{equation}
Prob \frac{\big(Chris \mid Email contains the words ``life deal'' \big)}{Constant}
\end{equation}
Then, find its complement. 

\end{itemize}

\subsection{Bayesian probabilities on your own}

\begin{itemize}
\item Prob \big(Chris $\mid$ Email contains the words ``love deal'' \big) = \\

Prob \big(``love deal'' $\mid$ C \big)*P\big(C \big) divided by a normalizing constant \\ 

where the constant is Prob \big(``love deal'' $\mid$ C \big) P\big(C) + Prob \big(``love deal'' $\mid$ S \big) P\big(S) \\

Then, take the complement.

\item Answers: 0.5555; 0.4444. 

\end{itemize}

\subsection{Why Is Naive Bayes Naive}

\begin{itemize}

\item Don't see underlying process (e.g. who is using the words) but get to see the outcome (e.g. the words that the person used). 

\item Answer: Word order is being ignored in Bayes Theorem whereas the words used and the length of the message (in terms of which words are used) are used. 

\end{itemize} 

\subsection{Naive Bayes Strengths and Weaknesses}

\begin{itemize}

\item Pros: Easy to implement, efficient

\item Cons: Can break (e.g. phrases with distinct meanings)

\item Good for text classification because can treat each word as a feature. 

\end{itemize}

\subsection{Congrats on Learning Naive Bayes}

\subsection{Lesson 2 Naive Bayes Mini-Project}

\begin{itemize}

\item See code in ''/Udacity\_Machine\_Learning/Lesson\_2\_Naive\_Bayes\_Mini\_Project\_Code.py''

\item: Answer to Quiz on Author ID Accuracy: no. of Chris training emails: 7936, no. of Sara training emails: 7884. Accuracy: 0.973833902162

\item Answer to Quiz on Timing Your NB Classifier: training time: 1.417 s, predicting time: 0.162 s. Training time is greater than the prediction time. 

\end{itemize}


\section{SVM, Support Vector Machines}

\subsection{Welcome to SVM}

\begin{itemize}

\item SVM is a very popular algorithm

\end{itemize}

\subsection{Quiz: Separating a Line}

\begin{itemize} 

\item SVM takes in data from 2+ classes as input and draws a line to separate the classes

\item Answer: The diagonal line that separates the Xs and the Os.

\end{itemize}

\subsection{Quiz: Choosing Between Separating Lines}

\begin{itemize} 

\item Answer: The vertical line is the best separator.

\end{itemize}


\subsection{Quiz: Choosing Between Separating Lines}

\begin{itemize} 

\item Want to choose a line that maximizes the distances to the nearest points in either class. Margin- maximizes distance to the nearest point.  

\item Answer: Something else because it maximizes the distances to the nearest points in either class (to be the most robust to classification errors). 

\end{itemize}

\subsection{Quiz: Practice with Margins}

\begin{itemize} 

\item Answer: The middle line maximizes the margin. It is the most robust to classification errors. 

\end{itemize}

\subsection{Quiz: SVMs and Tricky Data Distributions}

\begin{itemize} 

\item Answer: The line that fully separates the red from the blue points (diagonally downward). 

\item SVM prioritizes correct classification over maximizing the margin. 

\end{itemize}

\subsection{Quiz: SVM Response to Outliers}

\begin{itemize} 

\item What happens when no decision surface exists that completely separates the classes of data?

\item Answer: Do the best it can

\end{itemize}

\subsection{Quiz: SVM Outlier Practice}

\begin{itemize} 

\item SVM ignores extreme outliers. 

\item Answer: The line on the right is the best separator. 

\end{itemize}

\subsection{Handoff to Katie}

\begin{itemize} 

\item Making your own SVM

\end{itemize}

\subsection{SVM in SKlearn}

\begin{itemize}

\item Code: Import statement, training data, training features, create classifier, fit classifier, do a prediction.

\item Important: import svm, classifier is svm.SVC()

\end{itemize}

\subsection{SVM Decision Boundary}

\begin{itemize}

\item Unlike with our Naive Bayes classifier, the SVM decision boundary will be a straight line. 

\end{itemize}

\subsection{SVM Coding Up the SVM}

\begin{itemize}

\item Answer: See "Lesson\_3\_Section\_12\_Coding\_Up\_the\_SVM for code. Also, the accuracy is 0.92, so it does better than the Naive Bayes classifier. 

\end{itemize}


\subsection{Nonlinear SVMs}

\begin{itemize}

\item SVM can do complicated shapes in the decision boundary. 

\end{itemize}


\subsection{Quiz: Nonlinear Data}

\begin{itemize}

\item Answer: No. Given our definition so far, SVMs will not separate this dataset.

\end{itemize}

\subsection{Quiz: A New Feature}

\begin{itemize}

\item Can put x squared and y squared into SVM in addition to x and y inputs. 

\item Answer: Yes, this is now separable. 

\end{itemize}


\subsection{Visualizing A New Feature}

\begin{itemize}

\item Can do a linear transformation, which means we can look at it in a new coordinate system. 

\end{itemize}

\subsection{Quiz: Separating with the New Feature}

\begin{itemize}

\item Answer: Yes, the data classes are now separable. 

\end{itemize}

\subsection{Quiz: Practice Making a New Feature}

\begin{itemize}

\item Answer: Using the absolute value of X makes the data classes separable. 

\end{itemize}


\subsection{Kernel Trick}

\begin{itemize}

\item Use kernels to change x,y input space to a much larger input (high dimensional) space. And this can lead to a non-linear separation for x, y

\end{itemize}


\subsection{Quiz: Playing Around with Kernel Trick}

\begin{itemize}

\item How to create an SVC and specify a kernel type

\item Answer: All + more. There are many kernel types. 

\end{itemize}


\subsection{Quiz: Kernel and Gamma}

\begin{itemize}

\item Parameters in machine learning- arguments passed when you create your classifier (prior to fitting)

\item Parameters for an SVM- kernel (e.g. linear, rbf); C; gamma

\item Answer: The plot on the far right best represents an SVM with a linear kernel and a gamma = 1.0. 

\end{itemize}

\subsection{Quiz: SVM C Parameter}

\begin{itemize} 

\item C- controls the tradeoff between a smooth decision boundary and classifying training points correctly. 

\item Answer: A smaller value of C will cause the optimizer to look for a larger margin (even if this means that the decision boundary will misclassify more points. Therefore, a large C means that you expect that you will get more training points correct. 

\end{itemize}

\subsection{Quiz: Gamma Parameter}

\begin{itemize}

\item Gamma- defines how far the influence of a single training example reaches. Low values = far reach (even the points far from the potential decision boundary get taken into account when evaluating where to put the decision boundary) and high values = close. 

\end{itemize}

\subsection{Quiz: Overfitting}

\begin{itemize}

\item Overfitting is a common problem in machine learning

\item Answer: C, Gamma, and the Kernel type are all parameters that can impact fit/overfitting.

\end{itemize}

\subsection{SVM Strengths and Weaknesses}

\begin{itemize}

\item SVM works well in complicated domains where there is a clear margin of separation.

\item SVM doesn't work well in large data sets or data sets with lots of noise (e.g. overlapping classes)

\end{itemize}

\subsection{Lesson 2 Naive Bayes Mini-Project}

\begin{itemize}

\item See code in ''/Udacity\_Machine\_Learning/Lesson\_3\_SVM\_Mini\_Project\_Code.py''

\item Answer to Quiz on SVM Author ID Accuracy: The accuracy is 0.984072810011. 

\item Answer to Quiz on SVM Author ID Timing: The training time is 144.617 s and the predicting time: 14.203 s. Therefore, SVM is slower than Naive Bayes in this case. 

\item Answer to Quiz on A Smaller Training Set: The accuracy is 0.884527872582. 

\item Answer to Quiz on Speed-Accuracy Tradeoff: The two cases that happen in real time, flagging credit card fraud and voice recognition. 

\item Answer to Quiz on Deploy an RBF Kernel: Interestingly, with this more complex kernel, our accuracy is lower, 0.616040955631. 

\item Answer to Quiz on Optimize C Parameter: The accuracy when C = 10 is 0.616040955631, C = 100 is 0.616040955631, when C = 1000 is 0.821387940842, when C = 10000 is 0.892491467577. This means that the accuracy is greatest when C is highest (10000). 

\item Answer to Quiz on Accuracy after Optimizing C: The accuracy when C = 10000 is 0.892491467577. Based on the definition of C given earlier in the lesson, greater C equals more complex decision boundaries. 

\item Answer to Quiz on Optimized RBF vs. Linear: The accuracy of the optimized RBF is 0.990898748578. 

\item Answer to Quiz on Extracting Predictions from An SVM: The SVM predicts 1 for element 10, 0 for element 26, and 1 for element 50.

\item Answer to Quiz on How Many Chris Emails Predicted: He is expected to have authored X emails. Note: when the partial training set is used, this answer is 1018 emails. 

\end{itemize}

\subsection{Final Thoughts on Deploying SVM}

\begin{itemize}

\item Generally, Naive Bayes classifiers are better for text than SVM.

\end{itemize}

\section{Lesson 4: Decision Trees}

\subsection{Welcome to Decision Trees}

\begin{itemize}

\item Decision trees are the 3rd supervised classification algorithm that we are going to cover in this course. 

\item Decision trees are used frequently in machine learning. 

\end{itemize}

\subsection{Quiz: Linearly Separable Data}

\begin{itemize}

\item The data in the plot are not linearly separable (would need 2 lines). 

\end{itemize}

\subsection{Quiz: Multiple Linear Questions}

\begin{itemize}

\item Decision trees allow you to ask multiple linear questions (one after another). 

\item Answer: No, if it is not windy, then you stop.

\end{itemize}

\subsection{Quiz: Constructing a Decision Tree First Split}

\begin{itemize}

\item Answer: The initial split should be a X less than 3. 

\end{itemize}

\subsection{Quiz: Constructing a Decision Tree 2nd Split}

\begin{itemize}

\item Answer: The second split should be on Y less than 2. 

\end{itemize}

\subsection{Quiz: Class Labels After Second Split}

\begin{itemize}

\item Answer: If Y is less than 2, then the class label is the red cross. 

\end{itemize}

\subsection{Quiz: Constructing a Decision Tree/ 3rd Split}

\begin{itemize}

\item Answer: The best split when X is less than 3 is Y = 4. 

\item We will use an algorithm to find the decision tree boundary(ies). 

\end{itemize}

\subsection{Quiz: Coding a Decision Tree}

\begin{itemize}

\item Answer: See ``Lesson\_4\_Section\_8\_Coding\_A\_Decision\_Tree for code to produce a plot with the data and decision boundary. 

\end{itemize}

\subsection{Quiz: Decision Tree Accuracy}

\begin{itemize}

\item See ``Lesson\_4\_Section\_9\_Decision\_Tree\_Accuracy.py'' code. 

\item Answer: The accuracy is 0.908.

\end{itemize}

\subsection{Quiz: Decision Tree Parameters}

\begin{itemize}

\item Many parameters that we can try to tune. 

\item We are first going to look at the min\_sample\_split. This might be helpful for our overfitting problem because we can control the amount of splits on the tree. This asks the question about how many samples can be in the node before we stop splitting. The default is 2 samples. 

\item Answer: 1. You would not be able to split a node (or leaf) with 1 sample when using the min\_sample\_split default of 2. 

\end{itemize}

\subsection{Quiz: Min Samples Split}

\begin{itemize}

\item Answer: The plot on the left (with a more complicated decision boundary) has min\_sample\_split = 2 and the plot on the right (with a ''cleaner'' decision boundary) has a  min\_sample\_split = 50. 

\end{itemize}

\subsection{Quiz: Decision Tree Accuracy}

\begin{itemize}

\item See "Lesson\_4\_Section\_12\_Decision\_Tree\_Accuracy.py'' code. 

\item Answer: The accuracy when min\_sample\_split = 2 is 0.908 and when min\_sample\_split = 50 is 0.912. 

\end{itemize}

\subsection{Data Impurity and Entropy}

\begin{itemize}

\item Entropy- controls how a decision tree decides where to split the data (measure of impurity in a bunch of examples). For example, in this data set, we are trying to determine whether the car should go fast or slow. However, there might also be a speed limit that is enforced at certain places, so even if the terrain warrants that we go fast, if there is an enforced speed limit, we will go slow. 

\end{itemize}

\subsection{Quiz: Minimizing Impurity in Split}

\begin{itemize}

\item Answer: The plot with speed limit on the x-axis has a higher purity than that plot with bumpiness on the x-axis. 

\end{itemize}

\subsection{Formula of Entropy}

\begin{itemize}

\item When all examples are in the same class, the entropy is 0. When all examples are spread across all classes, the entropy is 1. 

\end{itemize}

\subsection{Quiz: Entropy Calculation Part 1}

\begin{itemize}

\item Answer: There are 2 slow observations. 

\end{itemize}

\subsection{Quiz: Entropy Calculation Part 2}

\begin{itemize}

\item Answer: There are 4 total observations. 

\end{itemize}

\subsection{Quiz: Entropy Calculation Part 3}

\begin{itemize}

\item Answer: 0.5 observations are slow. 

\end{itemize}

\subsection{Quiz: Entropy Calculation Part 4}

\begin{itemize}

\item Answer: 0.5 observations are fast. 

\end{itemize}

\subsection{Quiz: Entropy Calculation Part 5}

\begin{itemize}

\item The entropy is -1(0.5*log2(0.5)*2) = 1. 

\end{itemize}

\subsection{Information Gain}

\begin{itemize}

\item Information gain is defined as the entropy(parent) - [weighted average]entropy(children)

\item The decision tree algorithm maximizes the information gain

\end{itemize}

\subsection{Quiz: Information Gain Calculation Part 1}

\begin{itemize}

\item Answer: 3 observations have a grade of ``steep''. 

\end{itemize}

\subsection{Quiz: Information Gain Calculation Part 2}

\begin{itemize}

\item Answer: The entropy of this node is 0 because all of the observations are in the same class (fast). 

\end{itemize}

\subsection{Quiz: Information Gain Calculation Part 3}

\begin{itemize}

\item Answer: p\_slow = 2/3 because 2/3 of the observations are slow. 

\end{itemize}

\subsection{Quiz: Information Gain Calculation Part 4}

\begin{itemize}

\item Answer: p\_fast = 1/3 because 1/3 of the observations are fast.  

\end{itemize}

\subsection{Quiz: Information Gain Calculation Part 5}

\begin{itemize}

\item Answer: Entropy = -1((2/3)*log2(2/3) + (1/3)*log2(1/3)) = 0.91829583405. 

\end{itemize}

\subsection{Quiz: Information Gain Calculation}

\begin{itemize}

\item Entropy of parent = 1

\item Entropy of children = (3/4)*(0.9184) + (1/4)*(0)

\item Information gain = Entropy(parent) - [weighted average]*entropy(children) = 1 - 0.6888 = 0.3112.

\item Answer: The information gain that we get if we split based on grade = 0.3112. 

\end{itemize}

\subsection{Quiz: Information Gain Calculation Part 7}

\begin{itemize}

\item Entropy of parent = -1*((1/2)*log2(1/2)*2) = 1

\item Answer: When you sort based on bumpiness, there is still one slow and one fast observation in the category of ``bumpy'' and one slow and one fast observation in the category of ''smooth''. The observations are still spread evenly throughout the classes. Therefore, the entropy of bumpy is 1. 

\end{itemize}

\subsection{Quiz: Information Gain Calculation Part 8}

\begin{itemize}

\item Answer: When you sort based on bumpiness, there is still one slow and one fast observation in the category of ``bumpy'' and one slow and one fast observation in the category of ''smooth''. The observations are still spread evenly throughout the classes. Therefore, the entropy of flat is 1. 


\end{itemize}

\subsection{Quiz: Information Gain Calculation Part 9}

\begin{itemize}

\item Entropy of the parent = 1

\item Weighted average of the entropy of the children = 1

\item Answer: The information gain is 0. (1-1). 

\end{itemize}

\subsection{Quiz: Information Gain Calculation Part 10}

\begin{itemize}

\item Answer: When you sort based on speed limit, there are 2 slow observations when there is an enforced speed limit and 2 fast observations when there is not an enforced speed limit. The observations are separated by class and therefore, the entropy is 0. The information gain is 1-0=1. 

\end{itemize}

\subsection{Tuning Criterion Parameter}

\begin{itemize}

\item Default criterion for the Decision Tree Classifer in sklearn is ``gini'' (the Gini impurity) but it can also support ''entropy'' for information gain. 

\end{itemize}

\subsection{Bias-Variance Dilemma}

\begin{itemize}

\item High bias- the machine learning algorithm doesn't learn/isn't highly affected by the training

\item High variance- the algorithm learns a lot on the data, to the point that it can't generalize to novel data input types. 

\item Ideally, you want something that learns from the training, but isn't highly limited by only the data types in the training. 

\end{itemize}

\subsection{DT Strengths and Weaknesses}

\begin{itemize}

\item Strengths- easy to use, results are clearly interpretable. Can build larger classifiers (ensemble methods). 

\item Limitations- prone to overfitting (so have to be careful with parameter tunes).   

\end{itemize}

\subsection{Decision Tree Mini-Project}

\begin{itemize}

\item See code in ``/Udacity\_Machine\_Learning/Lesson\_4\_Decision\_Tree\_Mini\_Project\_Code.py''

\item Answer to Quiz on Your First Email DT: The accuracy is 0.978953356086. 

\item Answer to Quiz on Speeding Up Via Feature Selection: There are 3,785 features in the training data. 

\item Answer to Quiz on Changing the Number of Features: When you change the percentile of training data from 10 to 1, the number of features is 379. 

\item Answer to Quiz on SelectPercentile and Comparing:  A large value for percentile lead to a more complex decision tree (because you now have to classify many more features). 

\item Answer to Quiz on Accuracy Using 1\% of Features: The accuracy is 0.966439135381. 

\end{itemize}

\section{Lesson 5: Choose Your Own Algorithm}

\subsection{Algorithm Options}

\begin{itemize}

\item Can choose between k nearest neighbors, adaboost, and random forest

\item k nearest neighbors- classic, simple, easy-to-follow

\item adaboost and random forest- are ensemble methods, which means they are meta classifiers build from decision trees

\end{itemize}

\subsection{Quiz: Choose-Your-Own-Algorithm}

\begin{itemize}

\item k nearest neighbors- training phase stores feature vectors and class labels of the training data. During classification, each of the points in the test data is assigned a label which is most frequent among a given number of training samples to that test point. The number of training samples is user-defined, k. Frequently used distance metrics include Euclidean distance (for continuous variables) and the overlap metric/Hamming distance (for discrete variables such as text classification). For example, if k = 10 and of the 10 training points closest distance-wise to the test point, 7 are classified as ``red'' and 3 are classified as ``blue'', the test (query) point will be classified as ``red''. 

\item Random forest- operate by constructing many decision trees during training and outputting the model of classes or mean prediction of the trees. This process helps to correct for the fact that decision trees often overfit (low bias but very high variance on) the training set.

\item See code in ``/Udacity\_Machine\_Learning/Lesson\_5\_Choose\_ \\
Your\_Own\_Algorithm\_Code\_Random\_Forests.py''. Accuracy of the classifier using the terrain data is 0.91200000000000003.

\end{itemize}



\end{document}

